{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salem Ameen, PhD student at Salford University\n",
    "\n",
    "###Â  Parkinsons Telemonitoring Data Set \n",
    "\n",
    "Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"07c3e0bd-57ca-407c-a701-d8c11a90a0ae\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#07c3e0bd-57ca-407c-a701-d8c11a90a0ae\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"07c3e0bd-57ca-407c-a701-d8c11a90a0ae\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '07c3e0bd-57ca-407c-a701-d8c11a90a0ae' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#07c3e0bd-57ca-407c-a701-d8c11a90a0ae\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#07c3e0bd-57ca-407c-a701-d8c11a90a0ae\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from numpy import *\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics, grid_search, cross_validation\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous file we have not change the data as there is no missing value or add any new parameters so it can be downloaded from the web directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -2] Name or service not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7b628d1107b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    299\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[1;32m    300\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[1;32m    302\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[1;32m    303\u001b[0m                            else compression)\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salm/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# XXX what error?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -2] Name or service not known>"
     ]
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_UPDRS = data['motor_UPDRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_UPDRS.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('motor_UPDRS', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop('total_UPDRS', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "dataset = data.values\n",
    "X = dataset[:,0:20].astype(float)\n",
    "Y = M_UPDRS.values\n",
    "print 'Number of examples',len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make developement dataset for genrilization and final testing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Extracting 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=0)\n",
    "X = X_train\n",
    "Y = y_train\n",
    "new_data      = X_train\n",
    "new_test_data = X_test\n",
    "print 'Number of training examples',len(X_train)\n",
    "print 'Number of validation examples',len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of training data is very small so we need to use cross validation instead of spareate another validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous visulization the data has no missing value but has many outlier and some sparisty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the parameters for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Set the parameters for bias-variance dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data and save both the model and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# According to the nature of the data spasity and there is outliter RobustScaler class is recomandit \n",
    "# but when I tried both of them the result seems is same\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) # use same transform for testing data\n",
    "X = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save transformed data\n",
    "np.save('./data/X_train', X_train)\n",
    "np.save('./data/y_train', y_train)\n",
    "np.save('./data/X_test', X_test)\n",
    "np.save('./data/y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the model scaler to the desk\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the model if it is work\n",
    "scalerTest = joblib.load('scaler.pkl')\n",
    "new_data_scaled = scalerTest.transform(new_test_data)\n",
    "print \" =================================================================>>>>> done\"\n",
    "print new_data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BayesianRidge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least Absolute Shrinkage and Selection Operator (or BayesianRidge for short) is a modification of linear regression, like BayesianRidge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coefficient values (also called the l1-norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choosing multiple hyperparameters of an estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "estimator = BayesianRidge()\n",
    "estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For parametr alpha\n",
    "#param_range = np.logspace(-10.0, 10.0)\n",
    "#param_range = np.logspace(-7, 7, 3)\n",
    "param_range = np.linspace(0,1000,100)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    BayesianRidge(), X, Y, param_name=\"alpha_1\", param_range=param_range,\n",
    "    cv=num_folds,  n_jobs=-1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with BayesianRidge\")\n",
    "plt.xlabel(\"lambda_2\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out changing alpha_1, alpha_2, lambda_1 and lambda_2 does not change the performance so in next step the search will find the best cominations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search and/or Random search to determine best compination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report optimal parameters\n",
    "from operator import itemgetter\n",
    "def report(grid_scores, n_top=5):\n",
    "    params = None\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Parameters with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.4f} (std: {1:.4f})\".format(\n",
    "              score.mean_validation_score, np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "        if params == None:\n",
    "            params = score.parameters\n",
    "    \n",
    "    return params\n",
    "\n",
    "# The most common value for the max number of features to look at in each split is sqrt(# of features)\n",
    "sqrtfeat = np.sqrt(X.shape[1]) \n",
    "# Simple grid test (162 combinations)\n",
    "grid_test1 = { \"alpha_1\"      : [-1.0,0, 0.000001, 1.0],\n",
    "                \"alpha_2\"      : [-1.0,0,0.000001, 1.0],\n",
    "               \"lambda_1\"      : [-1.0,0,0.000001, 1.0],\n",
    "               \"lambda_2\"      : [-1.0,0,0.000001, 1.0]}\n",
    "\n",
    "# Large randomized test using max_depth to control tree size (5000 possible combinations)\n",
    "random_test1 = { \"alpha_1\"          : np.linspace(0,1.0,100),\n",
    "                  \"alpha_2\"      : np.linspace(0,1.0,100),\n",
    "                  \"lambda_1\"      : np.linspace(0,1.0,100),\n",
    "                  \"lambda_2\"      : np.linspace(0,1.0,100)}\n",
    "\n",
    "\n",
    "forest = BayesianRidge()\n",
    "\n",
    "print \"Hyperparameter optimization using GridSearchCV...\"\n",
    "grid_search = GridSearchCV(forest, grid_test1, n_jobs=-1, cv=num_folds)\n",
    "grid_search.fit(X, Y)\n",
    "best_params_from_grid_search = report(grid_search.grid_scores_)\n",
    "print '+++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "print \"Hyperparameter optimization using RandomizedSearchCV\"\n",
    "grid_search = RandomizedSearchCV(forest, random_test1, n_jobs=-1, cv=num_folds, n_iter=5)\n",
    "grid_search.fit(X, Y)\n",
    "best_params_from_rand_search1 = report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters Parameters: Parameters: {'alpha_2': 1.0, 'lambda_1': 1.0, 'lambda_2': -1.0, 'alpha_1': -1.0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, what weâre doing it training the exact same model with increasingly large fractions of our total training data, and plotting the error of the training and test sets at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "model = BayesianRidge(lambda_1 = 1.0, lambda_2 = -1.0, alpha_1 = -1.0, alpha_2=-1.0)\n",
    "title = \"Learning Curves (BayesianRidge Refression)\"\n",
    "cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=100,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "plot_learning_curve(model, title, X, Y, ylim=(0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More data from learning curve to show the process of building the model\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "     model, X, Y, train_sizes=[50, 80, 110], cv=cv, n_jobs=4)\n",
    "\n",
    "print 'train_sizes = ', train_sizes\n",
    "\n",
    "print ' training scores'\n",
    "print ' 50 training sizees'\n",
    "print train_scores[0].mean(),'Â±',train_scores[0].std()\n",
    "print ' 80 training sizees'\n",
    "print train_scores[1].mean(),'Â±',train_scores[1].std()\n",
    "print ' 110 training sizees'\n",
    "print train_scores[2].mean(),'Â±',train_scores[2].std()\n",
    "\n",
    "print '\\n validation scores'\n",
    "print ' 50 validation scores'\n",
    "print valid_scores[0].mean(),'Â±',valid_scores[0].std()\n",
    "print ' 80 validation scores'\n",
    "print valid_scores[1].mean(),'Â±',valid_scores[1].std()\n",
    "print ' 110 validation scores'\n",
    "print valid_scores[2].mean(),'Â±',valid_scores[2].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_cols = list(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print 'Scoring according to mean_squared_error'\n",
    "print results\n",
    "print '\\n Mean      Standard deviation'\n",
    "print(\"%.2f      %.2f\" % (results.mean(), results.std()))\n",
    "## Fit the model\n",
    "model.fit(X,Y)\n",
    "print '\\n print intercept and coefficients'\n",
    "print model.intercept_\n",
    "print model.coef_\n",
    "# pair the feature names with the coefficients\n",
    "print '\\n(feature, coefficients)'\n",
    "print zip(feature_cols, model.coef_)\n",
    "# Linear qu. of the model\n",
    "\n",
    "print (\"\\n Predicted Target = %.2f +(%.2f)X1 +(%.2f)X2 +(%.2f)X3 +(%.2f)X4+(%.2f)X4 +(%.2f)X6 +(%.2f)X6 +(%.2f)X8+(%.2f)X9+(%.2f)X10 +(%.2f)X11+(%.2f)X12 +(%.2f)X20\"\n",
    "       % (model.intercept_, model.coef_[0], model.coef_[1],model.coef_[2],model.coef_[3],\n",
    "       model.coef_[4],model.coef_[5],model.coef_[6],model.coef_[7],\n",
    "      model.coef_[8], model.coef_[9],model.coef_[10],model.coef_[11],\n",
    "      model.coef_[12]))\n",
    "print (\"\\n Predicted Target = %.2f +(%.2f)%s +(%.2f)%s +(%.2f)%s +(%.2f)%s+(%.2f)%s +(%.2f)%s +(%.2f)%s +(%.2f)%s+(%.2f)%s+(%.2f)%s +(%.2f)%s +(%.2f)%s +(%.2f)%s\"\n",
    "       % (model.intercept_, model.coef_[0], feature_cols[0], model.coef_[1], feature_cols[1],\n",
    "          model.coef_[2], feature_cols[2],model.coef_[3], feature_cols[3],\n",
    "       model.coef_[4], feature_cols[4],model.coef_[5], feature_cols[5],model.coef_[6], feature_cols[6],\n",
    "          model.coef_[7], feature_cols[7],\n",
    "      model.coef_[8], feature_cols[8], model.coef_[9], feature_cols[9],\n",
    "          model.coef_[10], feature_cols[10],model.coef_[11], feature_cols[11],\n",
    "      model.coef_[12], feature_cols[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise these valrible is not the input feature as descripe on the dataset, those varibles in scaled virsion on the orginal onces. So, to be more correct I should assign to them another varibles but for easy to unserstand I kept them as they were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients Of Determination {Calculating R^2}\n",
    "\n",
    "R^2, Close to one is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = cross_val_predict(model, X, Y, cv=kfold)\n",
    "r_squared = r2_score(Y, predicted, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = cross_val_predict(model, X, Y, cv=kfold)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y, predicted)\n",
    "ax.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotining using bokeh\n",
    "#TOOLS = [BoxSelectTool(), HoverTool()]\n",
    "#TOOLS = 'hover,box_zoom,box_select,crosshair,resize,reset,BayesianRidge_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "p = figure(plot_width=500, plot_height=500, tools=TOOLS)\n",
    "p.scatter(Y, predicted,x=\"jjj\", marker=\"o\", color=\"#80B1D3\", line_width=3)\n",
    "p.title.align = \"center\"\n",
    "p.xaxis.axis_label = \"Measured\"\n",
    "p.xaxis.axis_label_text_color = \"#aa6666\"\n",
    "#p.xaxis.axis_label_standoff = 30\n",
    "p.xaxis.major_tick_line_color = \"firebrick\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.xaxis.minor_tick_line_color = \"orange\"\n",
    "p.yaxis.axis_label = \"Predicted\"\n",
    "p.yaxis.axis_label_text_font_style = \"italic\"\n",
    "p.yaxis.axis_label_text_color = \"#aa6666\"\n",
    "p.background_fill_color = \"beige\"\n",
    "p.background_fill_alpha = 0.5\n",
    "#p.line([Y.min(), Y.max()], [Y.min(), Y.max()], line_dash=\"4 4\", line_width=2, color='red')\n",
    "\n",
    "# create a new plot with the toolbar below\n",
    "# line_dash=\"4 4\", line_width=2, color='red'\n",
    "p.line([Y.min(), Y.max()], [Y.min(), Y.max()], line_dash=\"4 4\", line_width=2, line_color=\"black\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(predicted, (predicted - Y), c='r', s=30)\n",
    "plt.title(\"Residual plot on the training data\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotining using bokeh\n",
    "#TOOLS = 'hover,box_zoom,box_select,crosshair,resize,reset,BayesianRidge_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "p = figure(plot_width=500, plot_height=500, title=\"Residual plot on the training data\", tools=TOOLS)\n",
    "p.scatter(predicted, (predicted - Y),x=\"jjj\", marker=\"o\", color=\"#80B1D3\", line_width=3)\n",
    "p.title.align = \"center\"\n",
    "p.xaxis.axis_label = \"Predicted\"\n",
    "p.xaxis.axis_label_text_color = \"#aa6666\"\n",
    "#p.xaxis.axis_label_standoff = 30\n",
    "p.xaxis.major_tick_line_color = \"firebrick\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.xaxis.minor_tick_line_color = \"orange\"\n",
    "p.yaxis.axis_label = \"Residuals\"\n",
    "p.yaxis.axis_label_text_font_style = \"italic\"\n",
    "p.yaxis.axis_label_text_color = \"#aa6666\"\n",
    "p.background_fill_color = \"beige\"\n",
    "p.background_fill_alpha = 0.5\n",
    "#p.line([2,38], [0, 0], line_dash=\"4 4\", line_width=2, color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best to deliver the model by pipeline the operation then at deployment can be done in one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  I used two ways to check the pipeline so any one can be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Pipeline\n",
    "# ANOVA \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "anova_filter = SelectKBest(f_regression, k=20)\n",
    "LR = BayesianRidge(lambda_1 = 1.0, lambda_2 = -1.0, alpha_1 = -1.0, alpha_2=-1.0)\n",
    "anova_LinReg = Pipeline([('scaler_Input',scaler), ('anova', anova_filter), ('LinReg', LR)])\n",
    "# You can set the parameters using the names issued\n",
    "# For instance, fit using a k of 10 in the SelectKBest\n",
    "anova_LinReg.set_params(anova__k=20).fit(new_data, Y)\n",
    "prediction = anova_LinReg.predict(new_data)\n",
    "print anova_LinReg.score(new_data, Y)                        \n",
    "# getting the selected features chosen by anova_filter\n",
    "results = cross_validation.cross_val_score(anova_LinReg, new_data, Y, cv=kfold, scoring=scoring)\n",
    "print 'Scoring according to mean_squared_error'\n",
    "print results\n",
    "print '\\n Mean      Standard deviation'\n",
    "print(\"%.2f      %.2f\" % (results.mean(), results.std()))\n",
    "print anova_LinReg.named_steps['anova'].get_support()\n",
    "r_squared = r2_score(Y, prediction, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using make_pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ANOVA SVM-C\n",
    "# 1) anova filter, take 3 best ranked features\n",
    "anova_filter = SelectKBest(f_regression, k=20)\n",
    "anova_BayesianRidge = make_pipeline(scaler, anova_filter, LR)\n",
    "anova_BayesianRidge.fit(new_data, Y)\n",
    "prediction = anova_BayesianRidge.predict(new_data)\n",
    "r_squared = r2_score(Y, prediction, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Pipeline model for developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the model scaler to the desk\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(anova_BayesianRidge, 'anova_BayesianRidge.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the model if it is work\n",
    "anova_BayesianRidgeTest = joblib.load('anova_BayesianRidge.pkl')\n",
    "NewPred = anova_BayesianRidgeTest.predict(new_data)\n",
    "print \" =================================================================>>>>> done\"\n",
    "r_squared = r2_score(Y, NewPred, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test on single data point, randomly choose point number 55\n",
    "print 'Point of training data before normalization\\n',new_data[55]\n",
    "print '\\nThe read target of the data = ', Y[55]\n",
    "NewPred = anova_BayesianRidgeTest.predict([new_data[55]])\n",
    "print '\\nThe Predicted target of the data = ', NewPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
