{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salem Ameen, PhD student at Salford University\n",
    "\n",
    "### Boston House price\n",
    "\n",
    "Kernel KernelRidge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"3c906686-ceb2-437a-8c68-78cf041fe4c5\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.1.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#3c906686-ceb2-437a-8c68-78cf041fe4c5\").text(\"BokehJS successfully loaded\");\n",
       "      var kernel = Jupyter.notebook.kernel\n",
       "      if (kernel.execute !== undefined) {\n",
       "          kernel.execute(\"import bokeh.io; bokeh.io._nb_loaded = True\");\n",
       "      }\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from numpy import *\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_selection\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics, grid_search, cross_validation\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous file we have not change the data as there is no missing value or add any new parameters so it can be downloaded from the web directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "feature_cols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv(url, sep='\\s+', names = feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples 506\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "dataset = data.values\n",
    "X = dataset[:,0:13].astype(float)\n",
    "Y = dataset[:,13]\n",
    "print 'Number of examples',len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 404\n",
      "Number of validation examples 102\n"
     ]
    }
   ],
   "source": [
    "# Make developement dataset for genrilization and final testing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Extracting 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=0)\n",
    "X = X_train\n",
    "Y = y_train\n",
    "new_data      = X_train\n",
    "new_test_data = X_test\n",
    "print 'Number of training examples',len(X_train)\n",
    "print 'Number of validation examples',len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of training data is very small so we need to use cross validation instead of spareate another validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous visulization the data has no missing value but has many outlier and some sparisty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking of the feature's importance to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM \t-5.66303451728\n",
      "ZN \t-4.05708578855\n",
      "INDUS \t-2.64681157725\n",
      "CHAS \t-5.35211936952\n",
      "NOX \t-0.662755471713\n",
      "RM \t0.308091165186\n",
      "AGE \t-1.49438692403\n",
      "DIS \t-0.879138192382\n",
      "RAD \t-3.59317788546\n",
      "TAX \t-1.65243007552\n",
      "PTRATIO \t-0.365905893853\n",
      "B \t0.0302622632502\n",
      "LSTAT \t-2.86735798245\n",
      "MEDV \t0.999999999922\n"
     ]
    }
   ],
   "source": [
    "# Using just correlation coefficient even in prevoius file we show them using data frame\n",
    "for col in feature_cols:\n",
    "    lm = KernelRidge()\n",
    "    lm.fit(data[[col]], data['MEDV'])\n",
    "    print col,'\\t', lm.score(data[[col]], data['MEDV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name      Score         Pvalue\n",
      "CRIM \t\t89.4861147577 \t1.17398708219e-19\n",
      "ZN \t\t75.257642299 \t5.71358415308e-17\n",
      "INDUS \t\t153.954883136 \t4.90025998175e-31\n",
      "CHAS \t\t15.9715124204 \t7.39062317052e-05\n",
      "NOX \t\t112.59148028 \t7.06504158625e-24\n",
      "RM \t\t471.846739876 \t2.48722887101e-74\n",
      "AGE \t\t83.4774592192 \t1.56998220919e-18\n",
      "DIS \t\t33.5795703259 \t1.20661172734e-08\n",
      "RAD \t\t85.9142776698 \t5.46593256965e-19\n",
      "TAX \t\t141.761356577 \t5.63773362769e-29\n",
      "PTRATIO \t\t175.105542876 \t1.60950947847e-34\n",
      "B \t\t63.0542291125 \t1.31811273408e-14\n",
      "LSTAT \t\t601.61787111 \t5.08110339439e-88\n",
      "MEDV \t\tinf \t0.0\n"
     ]
    }
   ],
   "source": [
    "# Randing the important features to the target\n",
    "model = feature_selection.SelectKBest(score_func=feature_selection.f_regression,k=4)\n",
    "results = model.fit(data[feature_cols], data['MEDV'])\n",
    "k=0\n",
    "print 'Feature Name      Score         Pvalue'\n",
    "for feat in feature_cols:\n",
    "    print feat,'\\t\\t',results.scores_[k],'\\t',results.pvalues_[k]\n",
    "    k=k+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the parameters for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Set the parameters for bias-variance dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data and save both the model and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# According to the nature of the data spasity and there is outliter RobustScaler class is recomandit \n",
    "# but when I tried both of them the result seems is same\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) # use same transform for testing data\n",
    "X = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save transformed data\n",
    "np.save('./data/X_train', X_train)\n",
    "np.save('./data/y_train', y_train)\n",
    "np.save('./data/X_test', X_test)\n",
    "np.save('./data/y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl', 'scaler.pkl_01.npy', 'scaler.pkl_02.npy', 'scaler.pkl_03.npy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model scaler to the desk\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =================================================================>>>>> done\n",
      "[[-0.40835869 -0.49960763 -1.12872913 ..., -0.71272928  0.18547577\n",
      "  -0.73610347]\n",
      " [ 0.71925111 -0.49960763  0.9988844  ...,  0.79267419  0.0831649\n",
      "  -0.4356916 ]\n",
      " [-0.40257488 -0.49960763  0.39610829 ..., -0.94082071  0.39472748\n",
      "  -0.30263246]\n",
      " ..., \n",
      " [-0.3982601   0.55937182 -0.85812418 ...,  0.56458276  0.41019833\n",
      "   0.06087961]\n",
      " [-0.39934279 -0.49960763 -0.07637654 ...,  0.0627816   0.30517724\n",
      "  -0.45626776]\n",
      " [-0.40088071 -0.49960763 -0.36702631 ...,  1.1120022   0.41166637\n",
      "  -0.05983383]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model if it is work\n",
    "scalerTest = joblib.load('scaler.pkl')\n",
    "new_data_scaled = scalerTest.transform(new_test_data)\n",
    "print \" =================================================================>>>>> done\"\n",
    "print new_data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KernelRidge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel KernelRidge regression (KRR) combines KernelRidge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choosing multiple hyperparameters of an estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kernel', 'degree', 'kernel_params', 'alpha', 'coef0', 'gamma']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "estimator = KernelRidge()\n",
    "estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-104:\n",
      "Process PoolWorker-107:\n",
      "Process PoolWorker-101:\n",
      "Process PoolWorker-103:\n",
      "Process PoolWorker-106:\n",
      "Process PoolWorker-108:\n",
      "Process PoolWorker-105:\n",
      "Process PoolWorker-102:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/Applications/Canopy.app/appdata/canopy-1.7.4.3348.macosx-x86_64/Canopy.app/Contents/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "    task = get()\n",
      "    task = get()\n",
      "    task = get()\n",
      "    task = get()\n",
      "    task = get()\n",
      "    task = get()\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "    task = get()\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 359, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "  File \"/Users/salemameen/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/externals/joblib/pool.py\", line 357, in get\n",
      "    racquire()\n",
      "    racquire()\n",
      "    racquire()\n",
      "    return recv()\n",
      "    racquire()\n",
      "    racquire()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# For parametr gamma\n",
    "#param_range = np.logspace(-10.0, 10.0)\n",
    "#param_range = np.logspace(-20, 2, 50)\n",
    "param_range = np.linspace(0.00000000001,0.5,10)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    KernelRidge(kernel='rbf'), X, Y, param_name=\"gamma\", param_range=param_range,\n",
    "    cv=num_folds,  n_jobs=-1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with KernelRidge\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks gamma between 0 and 0.1 works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For parametr alpha\n",
    "#param_range = np.logspace(-10.0, 10.0)\n",
    "#param_range = np.logspace(-20, 2, 50)\n",
    "param_range = np.linspace(0,0.2,100)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    KernelRidge(kernel='rbf'), X, Y, param_name=\"alpha\", param_range=param_range,\n",
    "    cv=num_folds,  n_jobs=-1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with KernelRidge\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.2, 1.0)\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks very small alpha works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search and/or Random search to determine best compination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report optimal parameters\n",
    "from operator import itemgetter\n",
    "def report(grid_scores, n_top=5):\n",
    "    params = None\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Parameters with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.4f} (std: {1:.4f})\".format(\n",
    "              score.mean_validation_score, np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "        if params == None:\n",
    "            params = score.parameters\n",
    "    \n",
    "    return params\n",
    "\n",
    "# The most common value for the max number of features to look at in each split is sqrt(# of features)\n",
    "sqrtfeat = np.sqrt(X.shape[1]) \n",
    "# Simple grid test (162 combinations)\n",
    "grid_test1 = { \"gamma\"      : np.linspace(0.00000000001,0.2,20),\n",
    "               \"alpha\"         :  np.linspace(0.00000000000001,0.2,20) }\n",
    "\n",
    "# Large randomized test using max_depth to control tree size (5000 possible combinations)\n",
    "random_test1 = { \"gamma\"          : [0.0000000001, 0.0000002, 0.00003],\n",
    "                 \"alpha\"         : [0.000001, 0.1] }\n",
    "\n",
    "\n",
    "forest = KernelRidge(kernel='rbf')\n",
    "\n",
    "print \"Hyperparameter optimization using GridSearchCV...\"\n",
    "grid_search = GridSearchCV(forest, grid_test1, n_jobs=-1, cv=num_folds)\n",
    "grid_search.fit(X, Y)\n",
    "best_params_from_grid_search = report(grid_search.grid_scores_)\n",
    "print '+++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "print \"Hyperparameter optimization using RandomizedSearchCV\"\n",
    "grid_search = RandomizedSearchCV(forest, random_test1, n_jobs=-1, cv=num_folds, n_iter=5)\n",
    "grid_search.fit(X, Y)\n",
    "best_params_from_rand_search1 = report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, what we’re doing it training the exact same model with increasingly large fractions of our total training data, and plotting the error of the training and test sets at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# 'alpha': 10.0, 'solver': 'lsqr'\n",
    "model = KernelRidge(kernel='rbf', gamma=0.052, alpha =  0.0216)\n",
    "title = \"Learning Curves (KernelRidge Refression)\"\n",
    "cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=100,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "plot_learning_curve(model, title, X, Y, ylim=(0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More data from learning curve to show the process of building the model\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "     model, X, Y, train_sizes=[50, 80, 110], cv=cv, n_jobs=4)\n",
    "\n",
    "print 'train_sizes = ', train_sizes\n",
    "\n",
    "print ' training scores'\n",
    "print ' 50 training sizees'\n",
    "print train_scores[0].mean(),'±',train_scores[0].std()\n",
    "print ' 80 training sizees'\n",
    "print train_scores[1].mean(),'±',train_scores[1].std()\n",
    "print ' 110 training sizees'\n",
    "print train_scores[2].mean(),'±',train_scores[2].std()\n",
    "\n",
    "print '\\n validation scores'\n",
    "print ' 50 validation scores'\n",
    "print valid_scores[0].mean(),'±',valid_scores[0].std()\n",
    "print ' 80 validation scores'\n",
    "print valid_scores[1].mean(),'±',valid_scores[1].std()\n",
    "print ' 110 validation scores'\n",
    "print valid_scores[2].mean(),'±',valid_scores[2].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print 'Scoring according to mean_squared_error'\n",
    "print results\n",
    "print '\\n Mean      Standard deviation'\n",
    "print(\"%.2f      %.2f\" % (results.mean(), results.std()))\n",
    "## Fit the model\n",
    "model.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise these valrible is not the input feature as descripe on the dataset, those varibles in scaled virsion on the orginal onces. So, to be more correct I should assign to them another varibles but for easy to unserstand I kept them as they were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients Of Determination {Calculating R^2}\n",
    "\n",
    "R^2, Close to one is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = cross_val_predict(model, X, Y, cv=kfold)\n",
    "r_squared = r2_score(Y, predicted, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = cross_val_predict(model, X, Y, cv=kfold)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y, predicted)\n",
    "ax.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotining using bokeh\n",
    "#TOOLS = [BoxSelectTool(), HoverTool()]\n",
    "#TOOLS = 'hover,box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "p = figure(plot_width=500, plot_height=500, tools=TOOLS)\n",
    "p.scatter(Y, predicted,x=\"jjj\", marker=\"o\", color=\"#80B1D3\", line_width=3)\n",
    "p.title.align = \"center\"\n",
    "p.xaxis.axis_label = \"Measured\"\n",
    "p.xaxis.axis_label_text_color = \"#aa6666\"\n",
    "#p.xaxis.axis_label_standoff = 30\n",
    "p.xaxis.major_tick_line_color = \"firebrick\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.xaxis.minor_tick_line_color = \"orange\"\n",
    "p.yaxis.axis_label = \"Predicted\"\n",
    "p.yaxis.axis_label_text_font_style = \"italic\"\n",
    "p.yaxis.axis_label_text_color = \"#aa6666\"\n",
    "p.background_fill_color = \"beige\"\n",
    "p.background_fill_alpha = 0.5\n",
    "#p.line([Y.min(), Y.max()], [Y.min(), Y.max()], line_dash=\"4 4\", line_width=2, color='red')\n",
    "\n",
    "# create a new plot with the toolbar below\n",
    "# line_dash=\"4 4\", line_width=2, color='red'\n",
    "p.line([Y.min(), Y.max()], [Y.min(), Y.max()], line_dash=\"4 4\", line_width=2, line_color=\"black\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(predicted, (predicted - Y), c='r', s=30)\n",
    "plt.title(\"Residual plot on the training data\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotining using bokeh\n",
    "#TOOLS = 'hover,box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "p = figure(plot_width=500, plot_height=500, title=\"Residual plot on the training data\", tools=TOOLS)\n",
    "p.scatter(predicted, (predicted - Y),x=\"jjj\", marker=\"o\", color=\"#80B1D3\", line_width=3)\n",
    "p.title.align = \"center\"\n",
    "p.xaxis.axis_label = \"Predicted\"\n",
    "p.xaxis.axis_label_text_color = \"#aa6666\"\n",
    "#p.xaxis.axis_label_standoff = 30\n",
    "p.xaxis.major_tick_line_color = \"firebrick\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.xaxis.minor_tick_line_color = \"orange\"\n",
    "p.yaxis.axis_label = \"Residuals\"\n",
    "p.yaxis.axis_label_text_font_style = \"italic\"\n",
    "p.yaxis.axis_label_text_color = \"#aa6666\"\n",
    "p.background_fill_color = \"beige\"\n",
    "p.background_fill_alpha = 0.5\n",
    "#p.line([2,38], [0, 0], line_dash=\"4 4\", line_width=2, color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best to deliver the model by pipeline the operation then at deployment can be done in one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  I used two ways to check the pipeline so any one can be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Pipeline\n",
    "# ANOVA \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "anova_filter = SelectKBest(f_regression, k=13)\n",
    "LRR = KernelRidge(kernel='rbf', gamma=0.052, alpha =  0.0216)\n",
    "anova_LinReg = Pipeline([('scaler_Input',scaler), ('anova', anova_filter), ('LinReg', LRR)])\n",
    "# You can set the parameters using the names issued\n",
    "# For instance, fit using a k of 10 in the SelectKBest\n",
    "anova_LinReg.set_params(anova__k=13).fit(new_data, Y)\n",
    "prediction = anova_LinReg.predict(new_data)\n",
    "print anova_LinReg.score(new_data, Y)                        \n",
    "# getting the selected features chosen by anova_filter\n",
    "results = cross_validation.cross_val_score(anova_LinReg, new_data, Y, cv=kfold, scoring=scoring)\n",
    "print 'Scoring according to mean_squared_error'\n",
    "print results\n",
    "print '\\n Mean      Standard deviation'\n",
    "print(\"%.2f      %.2f\" % (results.mean(), results.std()))\n",
    "print anova_LinReg.named_steps['anova'].get_support()\n",
    "r_squared = r2_score(Y, prediction, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using make_pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ANOVA SVM-C\n",
    "# 1) anova filter, take 3 best ranked features\n",
    "anova_filter = SelectKBest(f_regression, k=13)\n",
    "anova_KernelRidge = make_pipeline(scaler, anova_filter, LRR)\n",
    "anova_KernelRidge.fit(new_data, Y)\n",
    "prediction = anova_KernelRidge.predict(new_data)\n",
    "r_squared = r2_score(Y, prediction, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Pipeline model for developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the model scaler to the desk\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(anova_KernelRidge, 'anova_KernelRidge.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the model if it is work\n",
    "anova_KernelRidgeTest = joblib.load('anova_KernelRidge.pkl')\n",
    "NewPred = anova_KernelRidgeTest.predict(new_data)\n",
    "print \" =================================================================>>>>> done\"\n",
    "r_squared = r2_score(Y, NewPred, multioutput='variance_weighted')\n",
    "print 'R-squared = ',r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test on single data point, randomly choose point number 55\n",
    "print 'Point of training data before normalization\\n',new_data[55]\n",
    "print '\\nThe read target of the data = ', Y[55]\n",
    "NewPred = anova_KernelRidgeTest.predict([new_data[55]])\n",
    "print '\\nThe Predicted target of the data = ', NewPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
